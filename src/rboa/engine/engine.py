"""
Engine definition

Written by DEIMOS Space S.L. (dibb)

module rboa
"""

# Import python utilities
import datetime
import uuid
import random
import os
from dateutil import parser
import tarfile
from shutil import copyfile
import errno
import json

# Import SQLalchemy entities
from sqlalchemy.exc import IntegrityError, InternalError
from sqlalchemy.sql import func
from sqlalchemy.orm import scoped_session

# Import debugging
from eboa.debugging import debug, race_condition

# Import datamodel
from eboa.datamodel.base import Session
from rboa.datamodel.reports import Report, ReportGroup, ReportStatus, ReportText, ReportDouble, ReportObject, ReportGeometry, ReportBoolean, ReportTimestamp
from rboa.datamodel.alerts import ReportAlert

# Import query interface
from eboa.engine.query import Query

# Import exceptions
from eboa.engine.errors import WrongPeriod, ErrorParsingDictionary, WrongValue, OddNumberOfCoordinates, FilePathDoesNotExist, DuplicatedValues, WrongGeometry
from rboa.engine.errors import WrongGenerationPeriod

# Import parsing module
import rboa.engine.parsing as parsing

# Import auxiliary functions
from eboa.engine.common_functions import insert_values, insert_alert_groups, insert_alert_cnfs

# Import auxiliary functions
from rboa.engine.functions import get_rboa_archive_path

# Import logging
from eboa.logging import Log

logging = Log(name = __name__)
logger = logging.logger

archive_path = get_rboa_archive_path()

exit_codes = {
    "OK": {
        "status": 0,
        "message": "The metadata of the report {} associated to the generator {} with version {} has been ingested correctly associated to {} alerts related to event/s, {} alerts related to explicit reference/s, {} alerts related to report/s and {} alerts related to sources/s"
    },
    "METADATA_INGESTION_STARTED": {
        "status": 1,
        "message": "The metadata of the report {} associated to the generator {} with version {} is going to be ingested"
    },
    "WRONG_REPORT_PERIOD": {
        "status": 2,
        "message": "The metadata of the report {} associated to the generator {} with version {} has a validity period which its stop ({}) is lower than its start ({})"
    },
    "WRONG_VALUE": {
        "status": 3,
        "message": "The metadata of the report {} associated to the generator {} with version {} defines a wrong value. The error was: {}"
    },
    "ODD_NUMBER_OF_COORDINATES": {
        "status": 4,
        "message": "The metadata of the report {} associated to the generator {} with version {} defines a wrong value. The error was: {}"
    },
    "FILE_NOT_VALID": {
        "status": 5,
        "message": "The metadata of the report {} does not pass the schema verification"
    },
    "WRONG_GEOMETRY": {
        "status": 6,
        "message": "The metadata of the report {} associated to the generator {} with version {} defines a wrong geometry. The exception raised has been the following: {}"
    },
    "DUPLICATED_VALUES": {
        "status": 7,
        "message": "The metadata of the report {} associated to the generator {} with version {} defines duplicated values. The exception raised has been the following: {}"
    },
    "PROCESSOR_DOES_NOT_EXIST": {
        "status": 8,
        "message": "The report {} was going to be generated with the processor {} which does not exist. Reported error: {}"
    },
    "GENERATION_ENDED_UNEXPECTEDLY": {
        "status": 9,
        "message": "The report {} was going to be generated by the generator {} but the processing ended unexpectedly with the error {}"
    },
    "METADATA_INGESTION_ENDED_UNEXPECTEDLY": {
        "status": 10,
        "message": "The metadata of the report {} was going to be ingested after its generation by the generator {} but the ingestion ended unexpectedly with the error {}"
    },
    "FILE_DOES_NOT_EXIST": {
        "status": 11,
        "message": "The report with path {} does not exist"
    },
    "REPORT_ARCHIVED": {
        "status": 12,
        "message": "The report {} associated to the generator {} with version {} has been archived"
    },
    "WRONG_GENERATION_PERIOD": {
        "status": 13,
        "message": "The metadata of the report {} associated to the generator {} with version {} has a generation period which its stop ({}) is lower than its start ({})"
    },
    "HTML_FILE_NOT_GENERATED": {
        "status": 14,
        "message": "The report {} was going to be generated by the generator {} but the processing did not succeed unexpectedly and the html file was not generated"
    }
}

class Engine():
    """Class for communicating with the engine of the rboa module

    Provides access to the logic for inserting, deleting and updating
    the information stored into the DDBB related to reports
    """

    def __init__(self, data = None):
        """
        Instantiation method

        :param data: data provided to be treat by the engine (default None)
        :type data: dict
        """
        if data == None:
            data = {}
        # end if
        self.data = data
        self.Scoped_session = scoped_session(Session)
        self.session = self.Scoped_session()
        self.session_progress = self.Scoped_session()
        self.query = Query(self.session)
        self.operation = None
    
        return

    def _validate_data(self, data, report = None):
        """
        Method to validate the data structure

        :param data: structure of data to validate
        :type data: dict 
        :param report: name of the report to be stored
        :type report: str
       """

        try:
            parsing.validate_data_dictionary(data)
        except ErrorParsingDictionary as e:
            if report != None:
                self._insert_report_without_metadata(report)
                self._insert_report_status(exit_codes["FILE_NOT_VALID"]["status"], error = True, message = str(e))
                self.session.commit()
            # end if
            logger.error(str(e))
            return False
        # end try

        return True

    def treat_data(self, data = None, report = None, validate = True):
        """
        Method to treat the data stored in self.data or received by parameter

        :param data: structure of data to treat
        :type data: dict 
        :param report: name of the report to be stored
        :type report: str
        :param validate: flag to indicate if the schema check has to be performed
        :type validate: bool

        :return: exit_codes for every operation with the associated information (generator and report)
        :rtype: list of dictionaries
        """
        if data != None:
            self.data = data
        # end if
        
        if validate:
            is_valid = self._validate_data(self.data, report = report)
            if not is_valid:
                # Log the error
                logger.error(exit_codes["FILE_NOT_VALID"]["message"].format(report))
                returned_information = {
                    "report": report,
                    "generator": None,
                    "status": exit_codes["FILE_NOT_VALID"]["status"]
                }
                return [returned_information]
            # end if
        # end if

        returned_values = []
        for self.operation in self.data.get("operations") or []:
            returned_value = -1

            if self.operation.get("mode") == "insert":
                returned_value = self._insert_data()
                returned_information = {
                    "report": self.operation.get("report").get("name"),
                    "generator": self.operation.get("report").get("generator"),
                    "status": returned_value
                }
                returned_values.append(returned_information)
            # end if
        # end for
        return returned_values

    #####################
    # INSERTION METHODS #
    #####################
    def _initialize_context_insert_data(self):
        # Initialize context
        self.report = None
        self.alert_cnfs = {}
        self.alert_groups = {}

        return

    @debug
    def _insert_data(self):
        """
        Method to insert the metadata of the report into the DDBB for an operation of mode insert
        """
        # Initialize context
        self._initialize_context_insert_data()
        
        # Insert the metadata
        # Insert report group
        self._insert_report_group()
        
        # Insert report metadata
        try:
            self._insert_report()
            self.metadata_ingestion_start = datetime.datetime.now()
            log_info = exit_codes["METADATA_INGESTION_STARTED"]["message"].format(
                self.report.name,
                self.report.generator, 
                self.report.generator_version)
            self._insert_report_status(exit_codes["METADATA_INGESTION_STARTED"]["status"],
                                       message = log_info)
            # Log that the ingestion of the metadata of the report file has been started
            logger.info(log_info)
        except WrongPeriod as e:
            self.session.rollback()
            self._insert_report_status(exit_codes["WRONG_REPORT_PERIOD"]["status"], error = True, message = str(e))
            # Log that the report file has a wrong specified validity as the stop is lower than the start
            logger.error(e)
            # Insert content in the DDBB
            self.report.content_json = json.dumps(self.operation)
            self.session.commit()
            return exit_codes["WRONG_REPORT_PERIOD"]["status"]
        # end try
        except WrongGenerationPeriod as e:
            self.session.rollback()
            self._insert_report_status(exit_codes["WRONG_GENERATION_PERIOD"]["status"], error = True, message = str(e))
            # Log that the report file has a wrong specified validity as the stop is lower than the start
            logger.error(e)
            # Insert content in the DDBB
            self.report.content_json = json.dumps(self.operation)
            self.session.commit()
            return exit_codes["WRONG_GENERATION_PERIOD"]["status"]
        # end try

        # Get the general report entry (processor = None, version = None, report group = PENDING_GENERATION)
        # This is when using the command rboa_generation.py for generation control purposes
        self.general_report_progress = self.session_progress.query(Report).join(ReportGroup).filter(Report.name == self.operation.get("report").get("name"),
                                                                                  ReportGroup.name == "PENDING_GENERATION").first()

        if not self.general_report_progress:
            self.general_report_progress = self.report_progress
        # end if

        if not self.operation.get("report").get("ingested") == "false":
            self.general_report_progress.processor_progress = 100
        # end if

        self._insert_ingestion_progress(40)

        # Insert alert groups
        self._insert_alert_groups()
        
        self._insert_ingestion_progress(50)

        # Insert alert configuration
        self._insert_alert_cnfs()

        self._insert_ingestion_progress(60)

        self.session.begin_nested()
        # Insert alerts
        self._insert_alerts()

        logger.debug("Alerts inserted for the report file {} associated to the generator {} with version {}".format(self.report.name, self.report.generator, self.report.generator_version))

        if not self.operation.get("report").get("ingested") == "false":
            try:
                self._archive_report()
                log_info = exit_codes["REPORT_ARCHIVED"]["message"].format(
                    self.report.name,
                    self.report.generator, 
                    self.report.generator_version)
                self._insert_report_status(exit_codes["REPORT_ARCHIVED"]["status"],
                                           message = log_info)
                # Log that the ingestion of the metadata of the report file has been started
                logger.info(log_info)
            except FilePathDoesNotExist as e:
                self.session.rollback()
                self._insert_report_status(exit_codes["FILE_DOES_NOT_EXIST"]["status"], error = True, message = str(e))
                # Log that the report file has a wrong specified validity as the stop is lower than the start
                logger.error(e)
                # Insert content in the DDBB
                self.session.commit()
                return exit_codes["FILE_DOES_NOT_EXIST"]["status"]
            # end try
        # end if
        
        # At this point all the information has been inserted, commit data twice as there was a begin nested initiated
        self.session.commit()
        self.session.commit()
        
        # Log that the file has been ingested correctly
        log = exit_codes["OK"]["message"].format(
            self.report.name,
            self.report.generator, 
            self.report.generator_version,
            0,
            0,
            0,
            0)
        self._insert_report_status(exit_codes["OK"]["status"],True, message = log)
        logger.info(log)

        # Indicate that the ingestion of metadata has finished
        self._insert_ingestion_progress(100)
        
        # Commit data
        self.session.commit()

        self.session.close()
        self.session = self.Scoped_session()

        return exit_codes["OK"]["status"]      

    @debug
    def _insert_report_group(self):
        """
        Method to insert the report group
        """
        report = self.operation.get("report")
        name = report.get("group")
        description = report.get("group_description")
        self.report_group = self.session.query(ReportGroup).filter(ReportGroup.name == name).first()
        if not self.report_group:
            id = uuid.uuid1(node = os.getpid(), clock_seq = random.getrandbits(14))
            self.report_group = ReportGroup(id, name, description)
            self.session.add(self.report_group)
            try:
                race_condition()
                self.session.commit()
            except IntegrityError:
                # The report group has been inserted between the
                # query and the insertion. Roll back transaction for
                # re-using the session
                self.session.rollback()
                # Get the stored DIM signature
                self.report_group = self.session.query(ReportGroup).filter(ReportGroup.name == name).first()
                pass
            # end try
        # end if
    
    @debug
    def _insert_report(self):
        """
        Method to insert the metadata of the report
        """
        report = self.operation.get("report")
        name = report.get("name")
        generation_mode = report.get("generation_mode")
        validity_start = report.get("validity_start")
        validity_stop = report.get("validity_stop")
        triggering_time = report.get("triggering_time")
        generation_start = report.get("generation_start")
        generation_stop = report.get("generation_stop")
        generator = self.operation.get("report").get("generator")
        generator_version = self.operation.get("report").get("generator_version")

        id = uuid.uuid1(node = os.getpid(), clock_seq = random.getrandbits(14))
        if parser.parse(validity_stop) < parser.parse(validity_start):
            # The validity period is not correct (stop > start)
            # Create Report for registering the error in the DDBB
            generation_progress = None
            if not self.operation.get("report").get("ingested") == "false":
                generation_progress = 100
            # end if

            self.report = Report(id, name, triggering_time, self.report_group, generation_mode = generation_mode, generator = generator, generator_version = generator_version, generation_progress = generation_progress)
            self.session.add(self.report)
            self.session.commit()

            raise WrongPeriod(exit_codes["WRONG_REPORT_PERIOD"]["message"].format(name, generator, generator_version, validity_stop, validity_start))
        # end if

        if parser.parse(generation_stop) < parser.parse(generation_start):
            # The generation period is not correct (stop > start)
            # Create Report for registering the error in the DDBB
            generation_progress = None
            if not self.operation.get("report").get("ingested") == "false":
                generation_progress = 100
            # end if

            self.report = Report(id, name, triggering_time, self.report_group, validity_start = validity_start, validity_stop = validity_stop, generation_mode = generation_mode, generator = generator, generator_version = generator_version, generation_progress = generation_progress)
            self.session.add(self.report)
            self.session.commit()

            raise WrongGenerationPeriod(exit_codes["WRONG_GENERATION_PERIOD"]["message"].format(name, generator, generator_version, generation_stop, generation_start))
        # end if

        generation_progress = None
        if not self.operation.get("report").get("ingested") == "false":
            generation_progress = 100
        # end if

        self.report = Report(id, name, triggering_time, self.report_group, generation_mode = generation_mode, validity_start = validity_start, validity_stop = validity_stop, generation_start = generation_start, generation_stop = generation_stop, generator = generator, generator_version = generator_version, generation_progress = generation_progress)
        
        self.session.add(self.report)
        self.session.commit()

        self.report_progress = self.session_progress.query(Report).filter(Report.report_uuid == self.report.report_uuid).first()

        # Insert values
        list_values = {}
        if "values" in report:
            entity_uuid = {"name": "report_uuid",
                           "id": id
            }
            self._insert_values(report.get("values"), entity_uuid, list_values)
        # end if

        # Bulk insert values
        try:
            if "objects" in list_values:
                self.session.bulk_insert_mappings(ReportObject, list_values["objects"])
            # end if
            if "booleans" in list_values:
                self.session.bulk_insert_mappings(ReportBoolean, list_values["booleans"])
            # end if
            if "texts" in list_values:
                self.session.bulk_insert_mappings(ReportText, list_values["texts"])
            # end if
            if "doubles" in list_values:
                self.session.bulk_insert_mappings(ReportDouble, list_values["doubles"])
            # end if
            if "timestamps" in list_values:
                self.session.bulk_insert_mappings(ReportTimestamp, list_values["timestamps"])
            # end if
            if "geometries" in list_values:
                try:
                    self.session.bulk_insert_mappings(ReportGeometry, list_values["geometries"])
                except InternalError as e:
                    self.session.rollback()
                    raise WrongGeometry(exit_codes["WRONG_GEOMETRY"]["message"].format(self.report.name, self.report.generator, self.report.generator_version, e))
            # end if
        except IntegrityError as e:
            self.session.rollback()
            raise DuplicatedValues(exit_codes["DUPLICATED_VALUES"]["message"].format(self.report.name, self.report.generator, self.report.generator_version, e))
        # end try

        return

    def _insert_values(self, values, entity_uuid, list_values, position = 0, parent_level = -1, parent_position = 0, positions = None):
        """
        Method to insert the values associated to reports

        :param values: list of values to be inserted
        :type values: list
        :param entity_uuid: identifier of the report
        :type entity_uuid: uuid
        :param list_values: list with the inserted values for later bulk ingestion
        :type list_values: list
        :param position: value position inside the structure of values
        :type position: int
        :param parent_level: level of the parent value inside the structure of values
        :type parent_level: int
        :param parent_position: position of the parent value inside the correspoding level of the structure of values
        :type parent_position: int
        :param positions: counter of the positions per level
        :type positions: dict
        """
        try:
            insert_values(values, entity_uuid, list_values, position, parent_level, parent_position, positions)
        except WrongValue as e:
            self.session.rollback()
            raise WrongValue(exit_codes["WRONG_VALUE"]["message"].format(self.report.name, self.report.generator, self.report.generator_version, str(e)))
        except OddNumberOfCoordinates as e:
            self.session.rollback()
            raise OddNumberOfCoordinates(exit_codes["ODD_NUMBER_OF_COORDINATES"]["message"].format(self.report.name, self.report.generator, self.report.generator_version, str(e)))
        # end try

        return
    
    @debug
    def _insert_report_status(self, status, final = False, error = False, message = None):
        """
        Method to insert the DIM processing status

        :param status: code indicating the status of the processing of the file
        :type status: int
        :param message: error message generated when the ingestion does not finish correctly
        :type message: str
        :param final: boolean indicated whether it is a final status or not. This is to insert the ingestion duration in case of final = True
        :type final: bool
        """
        id = uuid.uuid1(node = os.getpid(), clock_seq = random.getrandbits(14))
        # Insert processing status
        status = ReportStatus(id,datetime.datetime.now(),status,self.report)
        self.session.add(status)

        if message:
            # Insert the error message
            status.log = message
        # end if

        # Flag the ingestion error
        self.report.generation_error = error

        if final:
            
            generated = True
            if self.operation.get("report").get("ingested") == "false":
                generated = False
            else:
                self._insert_ingestion_progress(100)
            # end if
            self.report.generated = generated

            if generated:
                # Insert ingestion duration and ingestion time
                self.report.metadata_ingestion_duration = datetime.datetime.now() - self.metadata_ingestion_start
            # end if
        # end if

        self.session.commit()

        return

    @debug
    def _insert_alert_groups(self):
        """
        Method to insert the groups of alerts
        """
        self.alert_groups = insert_alert_groups(self.session, self.operation)

        return

    @debug        
    def _insert_alert_cnfs(self):
        """
        Method to insert the alert configurations
        """
        self.alert_cnfs = insert_alert_cnfs(self.session, self.operation, self.alert_groups)
        
        return

    @debug
    def _insert_alerts(self):
        """
        Method to insert the alerts
        """
        list_alerts = []
        for alert in self.operation.get("alerts") or []:
            id = uuid.uuid1(node = os.getpid(), clock_seq = random.getrandbits(14))
            alert_cnf = self.alert_cnfs[alert.get("alert_cnf").get("name")]
            message = alert.get("message")
            generator = alert.get("generator")
            notification_time = alert.get("notification_time")

            kwargs = {}
            kwargs["message"] = message
            kwargs["ingestion_time"] = datetime.datetime.now()
            kwargs["generator"] = generator
            kwargs["notification_time"] = notification_time
            kwargs["alert_uuid"] = alert_cnf.alert_uuid
            kwargs["report_uuid"] = self.report.report_uuid
            kwargs["report_alert_uuid"] = id

            # Insert the alert into the list for bulk ingestion
            list_alerts.append(dict(kwargs))

        # end for
            
        # Bulk insert alerts
        if len(list_alerts) > 0:
            self.session.bulk_insert_mappings(ReportAlert, list_alerts)
        # end if

        return
    
    def _insert_report_without_metadata(self, name):
        """
        Method to insert the DIM processing but without an associated dim_signature

        :param name: name of the report
        :type name: str
        """
        # Create report group PENDING_GENERATION if it does not exist
        self.report_group = self.session.query(ReportGroup).filter(ReportGroup.name == "PENDING_GENERATION").first()
        if not self.report_group:
            id = uuid.uuid1(node = os.getpid(), clock_seq = random.getrandbits(14))
            self.report_group = ReportGroup(id, "PENDING_GENERATION")
            self.session.add(self.report_group)
            try:
                race_condition()
                self.session.commit()
            except IntegrityError:
                # The report group has been inserted between the
                # query and the insertion. Roll back transaction for
                # re-using the session
                self.session.rollback()
                # Get the stored DIM signature
                self.report_group = self.session.query(ReportGroup).filter(ReportGroup.name == name).first()
                pass
            # end try
        # end if

        id = uuid.uuid1(node = os.getpid(), clock_seq = random.getrandbits(14))
        self.report = Report(id, name, datetime.datetime.now().isoformat(), self.report_group)
        self.session.add(self.report)
        self.session.commit()

        return

    def _insert_ingestion_progress(self, progress):
        """
        Method to insert the ingestion progress of a report
        :param progress: value of progress
        :type progress: float
        """
        if not self.operation.get("report").get("ingested") == "false":
            self.report_progress.metadata_ingestion_progress = progress
            self.general_report_progress.metadata_ingestion_progress = progress
            self.session_progress.commit()
        # end if

        return

    def _archive_report(self):
        """
        Method to archive the specified report
        """
        report = self.operation.get("report")
        path = report.get("path")
        file_name = self.report.name
        file_name_no_extension, extension = os.path.splitext(file_name)
        compress = report.get("compress")
        triggering_time = self.report.triggering_time
        report_uuid = self.report.report_uuid
        
        # Check if file exists
        if not os.path.isfile(path):
            self.session.rollback()
            raise FilePathDoesNotExist(exit_codes["FILE_DOES_NOT_EXIST"]["message"].format(path))
        # end if

        do_compression = True
        if compress.lower() == "false":
            do_compression = False
        # end if

        year = str(triggering_time.year)
        month = "{:02d}".format(triggering_time.month)
        day = "{:02d}".format(triggering_time.day)

        try:
            os.makedirs(archive_path + "/" + year + "/" + month + "/" + day)
        except OSError as exc:
            if exc.errno != errno.EEXIST:
                raise
            # end if
            pass
        # end try
        
        if do_compression:
            relative_path = year + "/" + month + "/" + day +  "/" + str(report_uuid) + "_" + file_name_no_extension + ".tgz"
            tar = tarfile.open(archive_path + "/" + relative_path, "w:gz")
            tar.add(path, arcname=file_name)
            tar.close()
        else:
            relative_path = year + "/" + month + "/" + day +  "/" + str(report_uuid) + "_" + file_name
            copyfile(path, archive_path + "/" + relative_path)
        # end if

        self.report.relative_path = relative_path
        self.report.compressed = do_compression
        
        return
    
    def close_session (self):
        """
        Method to close the session
        """
        self.session.close()
        self.session_progress.close()
        return

def insert_report_status(session, report, status, error = False, message = None):
    """
    Method to insert the report status

    :param status: code indicating the status to be associated to the report
    :type status: int
    :param message: error message generated when the report generation does not finish correctly
    :type message: str
    """
    id = uuid.uuid1(node = os.getpid(), clock_seq = random.getrandbits(14))
    # Insert status
    status = ReportStatus(id,datetime.datetime.now(),status,report)
    session.add(status)

    if message:
        # Insert the error message
        status.log = message
    # end if

    if error:
        # Flag the report generation error
        report.generation_error = True
    # end if

    session.commit()

    return
